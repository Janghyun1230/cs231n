{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.탠서플로우 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\LG\\Documents\\Deep_learning\\cs231n\\assignment2\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.0, 8.0]\n",
      "[1.0, 2.0, 86.0]\n",
      "[2.2351741790771484e-08, 1.640000008046627, 10.758400838708896]\n",
      "[-0.32799998533725705, 1.5088000058650972, 2.43570194621919]\n",
      "[-0.4329599929618832, 1.4536960036950113, 1.4887147192318073]\n",
      "[-0.46392319723167397, 1.4239923226557161, 1.3552971046234983]\n",
      "[-0.47036774327976599, 1.402857576761992, 1.3122328897679232]\n",
      "[-0.46871861270116572, 1.3847025196827829, 1.2798015282481003]\n",
      "[-0.46442794911292667, 1.3676700410290989, 1.2492011176645912]\n",
      "[-0.45930518796549308, 1.3511420279387862, 1.2194459272006168]\n",
      "[-0.45395048089364148, 1.3349117036595579, 1.1904120028399734]\n"
     ]
    }
   ],
   "source": [
    "# constant는 상수, Variable 은 변수 (ex weight) , placeholder 는 입력값 (ex X)\n",
    "\n",
    "# 그래프 그리기\n",
    "a = tf.Variable(1, dtype = tf.float64)\n",
    "b = tf.Variable(2, dtype = tf.float64)\n",
    "x = tf.placeholder(dtype = tf.float64)\n",
    "y = tf.placeholder(dtype = tf.float64)\n",
    "linear = a*x + b\n",
    "\n",
    "loss1 = tf.square(y-linear)\n",
    "loss = tf.reduce_sum(loss1)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [0,0,0,0]\n",
    "\n",
    "# 실행하기 (각 노드별로 ex da, db는 loss라는 함수의 Variable a, b의 gradient인 노드 \n",
    "# sess.run([da,db], {x : 2,y : 0}) 은 위 두 노드를 활성화하는 것. variable은 initialize로 초기값으로 설정되어있고 입력값을 입력해주어야\n",
    "# 그 지점에서의 node를 계산한다. \n",
    "\n",
    "with tf.Session() as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "    da, db = tf.gradients(loss, [a,b])\n",
    "    print(sess.run([da, db], {x : 2, y : 0}))\n",
    "    for i in range(10):\n",
    "        print(sess.run([a,b,loss],{x:x_train,y:y_train}))\n",
    "        sess.run(train,{x:x_train,y:y_train})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64), array([-2.        , -1.78947368, -1.57894737, -1.36842105, -1.15789474,\n",
      "       -0.94736842, -0.73684211, -0.52631579, -0.31578947, -0.10526316,\n",
      "        0.10526316,  0.31578947,  0.52631579,  0.73684211,  0.94736842,\n",
      "        1.15789474,  1.36842105,  1.57894737,  1.78947368,  2.        ]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\", reuse = False) :\n",
    "    b = tf.get_variable(\"b\", [10], dtype = tf.float32)\n",
    "    \n",
    "with tf.variable_scope(\"foo\", reuse = True) :\n",
    "    b = tf.get_variable(\"b\", dtype = tf.float32)    \n",
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "a = np.array(b.eval())\n",
    "print(np.histogram(a, bins = np.linspace(-2,2,20)))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 그리기 및 함수 안에서 변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 함수 안에 변수가 입력되어 사용하지 못하여도 graph에는 그려진다!!!\n",
    "# 이게 좀 희안한게 함수안에서 변수를 설정해도 그래프상에는 남아서 변수가 존재하긴하는데 사용을 못함 ex) run이 안됨\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def f():\n",
    "    init = tf.constant_initializer(1.0)\n",
    "    a = tf.get_variable(name= 'a', shape = [2,2], dtype = tf.float32, initializer = init)\n",
    "    b = tf.nn.softmax(a)\n",
    "    c = tf.reduce_sum(a)\n",
    "    return c\n",
    "\n",
    "c = f()\n",
    "#d = a    #에러가 안뜬다...\n",
    "\n",
    "# summary할 값을 설정하고 이를 summary.merge로 객체를 생성 (scalar는 실수 histogram 은 행렬)\n",
    "c_summary = tf.summary.scalar('c',c)\n",
    "c_merged = tf.summary.merge([c_summary])\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    #summary writer\n",
    "    writer = tf.summary.FileWriter(\"./tensorflow/new/ex_sum3\", sess.graph)\n",
    "    \n",
    "    # merged 객체 실행 후 writer에 입력\n",
    "    summary = sess.run(c_merged)\n",
    "    writer.add_summary(summary, global_step = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train\n",
    "\n",
    "1. opimizer\n",
    "<br>    1) tf.train.AdamOptimizer(learning_rate = 1e-3, beta1, beta2, epsilon)\n",
    "<br>    2) tf.train.RMSPropOptimizer(learning_rate, decay, momentum, epsilon)\n",
    "<br>    *learning rate만 입력해주면 됨. 다른것은 설정되어있음.(변경해서 테스트 가능)\n",
    "<br>    \n",
    "2. learning rate decay\n",
    "<br>    tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase = False)\n",
    "<br>    output = learning_rate*decay_rate^(global_step/decay_steps)\n",
    "<br>    staircase True => 위 지수가 정수로 표현됨\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### tf.nn\n",
    "1. tf.nn.conv2d(input, filter, strides, padding)\n",
    "<br>   input = [N,H,W,C] \n",
    "<br>   filter = [h,w,C,F]\n",
    "<br>   strides = [1,s,s,1]\n",
    "<br>   padding = 'SAME' or 'VALID' (SAME이 입력, 출력 크기를 같게 유지)\n",
    "   \n",
    "2. tf.nn.batchnormalization & tf.nn.batch_norm_with_global_normalization(t,m,v,beta,gamma,variance_epsilon, scale_after_normalization)\n",
    "<br>    t : input 4D\n",
    "<br>    m, v = tf.nn.moments(x, axes=[0,1,2])\n",
    "<br>    beta(initialize with 0), gamma(1) = filter size tensor\n",
    "<br>    scale_after_normalization = True\n",
    "\n",
    "    tf.contrib.layers.batch_norm is better!!!\n",
    "    \n",
    "\n",
    "2. tf.nn.relu(input) usually input =  conv_output + bias\n",
    "\n",
    "3. tf.nn.maxpool(value, ksize, strides, padding)\n",
    "<br>   value = input\n",
    "<br>   ksize = window size [1,2,2,1]\n",
    "<br>   strides = usually [1,2,2,1]\n",
    "<br>   padding 위와 같음\n",
    "\n",
    "4. tf.nn.dropout(x, keep_prob)\n",
    "<br>   output is multiplied by 1/keep_prob\n",
    "\n",
    "5. fully connected 는 matmul을 이용 tf.matmul(input, W) + bias\n",
    "<br>   tf.contrib.layers.fully_connected 의 경우 input과 num_output을 입력해주면 weights와 biases를 생성\n",
    "<br>   초기 init은 각각 xavier, zero, activation_fn은 tf.nn.relu가 default, normalizer_fn, normalizer_params를 입력해주면 \n",
    "<br>   ex)tf.contrib.layers.batch_norm, normalizer_params = {'scale' = True} 하면 batch를 하고 activation fn사용\n",
    "\n",
    "6. tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding)\n",
    "<br>   그냥 conv를 행렬로 표현하여 연산가능한데 이를 transpose한 연산을 적용해주는 것이라 생각하면 된다.\n",
    "<br>   conv2d(output,filter, stride, padding)을 넣어준것을 반대로 해주는 것. 변수는 그대로 입력하여주면 된다.\n",
    "    \n",
    "\n",
    "\n",
    "1. tf.nn.softmax(logits)\n",
    "<br>    logits : 2차원 행렬 dtype = float32 or 64\n",
    "<br>    output = scores\n",
    "    \n",
    "2. tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "<br>    labels : class probability matrix\n",
    "<br>    (tip : use tf.one_hot(indices, depth) indices : labels, depth : num_classes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.MNIST 다루기 (데이터 로드 후 conv net으로 classification)\n",
    "2-1. 데이터는 lecun homepage에서 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train set is done\n",
      "10000 test set is done\n",
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import data_mnist\n",
    "\n",
    "X_train, y_train, X_test, y_test = data_mnist.mnist()\n",
    "\n",
    "X_train_data = np.array(X_train)\n",
    "y_train_label = np.array(y_train)\n",
    "X_test_data = np.array(X_test)\n",
    "y_test_label = np.array(y_test)\n",
    "\n",
    "print(X_train_data.shape)\n",
    "print(y_train_label.shape)\n",
    "print(X_test_data.shape)\n",
    "print(y_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20a0de0df28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_data[0], cmap = cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. train conv net\n",
    "in window I should use tensorboard --logdir=E:\\Users\\LG\\Documents\\CNN\\assignment2\\tensorflow\\new\\ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1000 iter accuracy = 0.095\n",
      "50 / 1000 iter accuracy = 0.89\n",
      "100 / 1000 iter accuracy = 0.93\n",
      "150 / 1000 iter accuracy = 0.96\n",
      "200 / 1000 iter accuracy = 0.965\n",
      "250 / 1000 iter accuracy = 0.985\n",
      "300 / 1000 iter accuracy = 0.97\n",
      "350 / 1000 iter accuracy = 0.96\n",
      "400 / 1000 iter accuracy = 0.98\n",
      "450 / 1000 iter accuracy = 0.97\n",
      "500 / 1000 iter accuracy = 0.985\n",
      "550 / 1000 iter accuracy = 0.995\n",
      "600 / 1000 iter accuracy = 0.985\n",
      "650 / 1000 iter accuracy = 0.97\n",
      "700 / 1000 iter accuracy = 0.985\n",
      "750 / 1000 iter accuracy = 0.99\n",
      "800 / 1000 iter accuracy = 0.99\n",
      "850 / 1000 iter accuracy = 1\n",
      "900 / 1000 iter accuracy = 0.985\n",
      "950 / 1000 iter accuracy = 0.99\n",
      "test accuracy = 0.9768\n"
     ]
    }
   ],
   "source": [
    "# should reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# reshape data set\n",
    "N = X_train_data.shape[0]\n",
    "X_train = X_train_data[:,:,:,np.newaxis]\n",
    "y_train = np.zeros((N,10))\n",
    "y_train[range(N),y_train_label] = 1\n",
    "M = X_test_data.shape[0]\n",
    "X_test = X_test_data[:,:,:,np.newaxis]\n",
    "y_test = np.zeros((M,10))\n",
    "y_test[range(M),y_test_label] = 1\n",
    "\n",
    "# set placeholders \n",
    "x = tf.placeholder(tf.float32, shape=[None, 28,28,1])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "reg = tf.placeholder(tf.float32)\n",
    "\n",
    "global_step = tf.Variable(0, trainable = 'False', name= 'global_step')\n",
    "\n",
    "# for simplication define variable func\n",
    "def weight_variable(shape, name, weight_power = 0.1):\n",
    "    return tf.Variable(tf.truncated_normal(shape, dtype = tf.float32, stddev = weight_power), name = name)\n",
    "def bias_variable(shape, name, bias = 0.1) :\n",
    "    return tf.Variable(tf.constant(bias, shape = shape), dtype = tf.float32, name = name)\n",
    "\n",
    "# choose mini batch\n",
    "def make_batch(x, y, batch_size) :\n",
    "    N = x.shape[0]\n",
    "    a = np.random.choice(N, batch_size)\n",
    "    return x[a], y[a]\n",
    "\n",
    "# set neural net\n",
    "x1 = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "with tf.name_scope('conv1') :   # conv batch relu pool\n",
    "    W_conv1 = weight_variable([5,5,1,32],'W1')\n",
    "    b_conv1 = bias_variable([32], 'b1')\n",
    "\n",
    "    out = tf.nn.conv2d(x1,W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1\n",
    "\n",
    "    gamma1 = tf.Variable(tf.ones([32]),dtype = tf.float32, name = 'gamma1')\n",
    "    beta1 = tf.Variable(tf.zeros([32]),dtype = tf.float32, name = 'beta')\n",
    "    \n",
    "    mean1, var1 = tf.nn.moments(out, axes = [0,1,2])\n",
    "    out = tf.nn.batch_norm_with_global_normalization(out, mean1, var1, beta1, gamma1, 1e-5, True)\n",
    "    \n",
    "    h_conv1 = tf.nn.relu(out)\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    W1_hist = tf.summary.histogram('W1', W_conv1)\n",
    "    b1_hist = tf.summary.histogram('b1', b_conv1)\n",
    "    var_merged = tf.summary.merge([W1_hist, b1_hist])\n",
    "\n",
    "with tf.name_scope('conv2') :   # conv batch relu pool\n",
    "    W_conv2 = weight_variable([5,5,32,64], 'b2')\n",
    "    b_conv2 = bias_variable([64], 'b2')\n",
    "    \n",
    "    out2 = tf.nn.conv2d(h_pool1,W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2\n",
    "\n",
    "    gamma2 = tf.Variable(tf.ones([64]),dtype = tf.float32, name = 'gamma2')\n",
    "    beta2 = tf.Variable(tf.zeros([64]),dtype = tf.float32, name = 'beta2')\n",
    "    \n",
    "    mean2, var2 = tf.nn.moments(out2, axes = [0,1,2])\n",
    "    out2 = tf.nn.batch_norm_with_global_normalization(out2, mean2, var2, beta2, gamma2, 1e-5, True)\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(out2)\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "with tf.name_scope('fc1') :    # fc relu drop(selective) fc\n",
    "    W_fc1 = weight_variable([7*7*64,1024], 'W3')\n",
    "    b_fc1 = bias_variable([1024], 'b3')\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1)+b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "with tf.name_scope('output') :    \n",
    "    W_fc2 = weight_variable([1024,10], 'W4')\n",
    "    b_fc2 = bias_variable([10], 'b4')\n",
    "    y_out = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "# create loss node\n",
    "reg_loss = tf\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y_out))\n",
    "\n",
    "loss_hist = tf.summary.scalar('loss',loss)\n",
    "loss_merged = tf.summary.merge([loss_hist])\n",
    "\n",
    "# create optimizer and attatch to loss \n",
    "optimizer = tf.train.AdamOptimizer(5e-4)\n",
    "train_step = optimizer.minimize(loss, global_step = global_step)\n",
    "\n",
    "# create accuracy node\n",
    "correct_prediction = tf.equal(tf.argmax(y_out,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))\n",
    "\n",
    "train_acc_hist = tf.summary.scalar('train_accuracy',accuracy)\n",
    "train_acc_merged = tf.summary.merge([train_acc_hist])\n",
    "test_acc_hist = tf.summary.scalar('test_accuracy',accuracy)\n",
    "test_acc_merged = tf.summary.merge([test_acc_hist])\n",
    "\n",
    "# if need let's save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # make writer object\n",
    "    writer = tf.summary.FileWriter(\"./tensorflow/new/exBN2\", sess.graph)\n",
    "    \n",
    "    for i in range(1000) :\n",
    "        batch = make_batch(X_train, y_train, 200)\n",
    "        \n",
    "        #each 10 iter calculate accuracy and write at tensorboard with W1, b1\n",
    "        if i % 50 ==0 :\n",
    "            train_acc = accuracy.eval(feed_dict = {x : batch[0], y:batch[1], keep_prob : 1.0})\n",
    "            \n",
    "            summary = sess.run(train_acc_merged, feed_dict = {x : batch[0], y:batch[1], keep_prob : 1.0})\n",
    "            writer.add_summary(summary, global_step=sess.run(global_step))\n",
    "            summary = sess.run(test_acc_merged, feed_dict = {x : X_test, y: y_test, keep_prob : 1.0})\n",
    "            writer.add_summary(summary, global_step=sess.run(global_step))\n",
    "            \n",
    "            summary = sess.run(var_merged)\n",
    "            writer.add_summary(summary, global_step=sess.run(global_step))\n",
    "            \n",
    "            print(\"%d / 1000 iter accuracy = %g\"%(i,train_acc))\n",
    "            \n",
    "        # write loss     \n",
    "        summary = sess.run(loss_merged,feed_dict = {x : batch[0], y:batch[1], keep_prob : 1.0})\n",
    "        writer.add_summary(summary, global_step=sess.run(global_step))\n",
    "        \n",
    "        # run a opimizer and update once\n",
    "        train_step.run(feed_dict = {x : batch[0], y:batch[1], keep_prob : 1.0})\n",
    "        \n",
    "    test_acc = accuracy.eval(feed_dict = {x : X_test, y: y_test, keep_prob : 1.0})\n",
    "    print(\"test accuracy = %g\"%test_acc)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Variable\n",
    "1) variable_scope\n",
    "2) get variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a name :  conv1/a:0\n",
      "b name:  conv1/a:0\n",
      "(array([[ 1.,  1.],\n",
      "       [ 1.,  1.]], dtype=float32), array([[ 1.,  1.],\n",
      "       [ 1.,  1.]], dtype=float32))\n",
      "global _variable : ['conv1/a:0', 'bias/bias:0', 'beta1_power:0', 'beta2_power:0', 'conv1/a/Adam:0', 'conv1/a/Adam_1:0']\n",
      "trainable_variable : ['conv1/a:0']\n",
      "scope bias variable : [<tf.Variable 'conv1/a:0' shape=(2, 2) dtype=float32_ref>]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('conv1') as v:\n",
    "    a = tf.get_variable('a', (2,2), initializer = tf.constant_initializer(1))\n",
    "    print(\"a name : \", a.name)\n",
    "    v.reuse_variables() # or tf.get_variable_scope() == v\n",
    "    with tf.variable_scope(v):\n",
    "        b = tf.get_variable('a', (2,2), initializer = tf.constant_initializer(2))\n",
    "        print(\"b name: \", b.name)\n",
    "with tf.variable_scope('bias') as l :\n",
    "    c = tf.get_variable('bias', [1], initializer = tf.random_normal_initializer(), trainable = False)\n",
    "\n",
    "loss = tf.reduce_mean(a)\n",
    "\n",
    "op = tf.train.AdamOptimizer(learning_rate = 1e-3)\n",
    "train_step = op.minimize(loss)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "print(sess.run((a,b)))\n",
    "print(\"global _variable :\",[v.name for v in tf.global_variables()])\n",
    "print(\"trainable_variable :\",[v.name for v in tf.trainable_variables()])\n",
    "print(\"scope bias variable :\", tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'conv1'))\n",
    "print(tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'conv1'))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.53314279,  0.0383955 ],\n",
      "       [-1.55440473, -0.8199383 ]]), array([-1.11786687], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "a = np.random.randn(2,2)\n",
    "b = tf.convert_to_tensor(a)\n",
    "c = tf.get_variable('c',[1])\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "print(sess.run([b, c]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. save and retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0706867   0.53510869]\n",
      " [ 0.80864751 -0.54257667]]\n",
      "(1.1826231, 0.27655828)\n",
      "model saved in file C:\\Users\\LG\\Documents\\Deep_learning\\cs231n\\assignment2\\tensorflow\\save\\save_ex.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def add(x):\n",
    "    with tf.variable_scope(\"conv\"):\n",
    "        a = tf.get_variable('a',[2,2])\n",
    "    mean, var = tf.nn.moments(a,axes=[0,1])\n",
    "    return mean+x, var\n",
    "\n",
    "b = add(1)\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    a1 = tf.trainable_variables()[0]\n",
    "    print(sess.run(a1))\n",
    "    print(sess.run(b))\n",
    "    save_path = saver.save(sess, r\"C:\\Users\\LG\\Documents\\Deep_learning\\cs231n\\assignment2\\tensorflow\\save\\save_ex.ckpt\")\n",
    "    print(\"model saved in file %s\"%save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\LG\\Documents\\Deep_learning\\cs231n\\assignment2\\tensorflow\\save\\save_ex.ckpt\n",
      "model retored\n",
      "[array([[-0.0706867 ,  0.53510869],\n",
      "       [ 0.80864751, -0.54257667]], dtype=float32), array([[ 0.9293133 ,  1.53510869],\n",
      "       [ 1.80864751,  0.45742333]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"conv\", reuse=False):\n",
    "    a = tf.get_variable('a',[2,2])\n",
    "\n",
    "# variable share\n",
    "with tf.variable_scope(\"conv\", reuse=True):\n",
    "    b = tf.get_variable('a',[2,2])+1\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# only variable name is important\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, r\"C:\\Users\\LG\\Documents\\Deep_learning\\cs231n\\assignment2\\tensorflow\\save\\save_ex.ckpt\")\n",
    "    print(\"model retored\")\n",
    "    print(sess.run([a,b]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([3,3])\n",
    "b = tf.reduce_mean((a-1)**2)/2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(0, dtype = tf.float32)\n",
    "a = tf.assign(x, 1)\n",
    "y= x+1    \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(\"./tensorflow/new/ex6\", sess.graph)\n",
    "    sess.run(x.value())\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for complex batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1.27279 (0.9+0.9j) (0.9-0.9j) 0.785398\n",
      "[ 1.+2.j  2.+2.j] (1.5+2j)\n",
      "(0.9+0.9j)\n",
      "[[[[ 0.+0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j\n",
      "     0.+0.j]]]]\n",
      "(1+0j)\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "from tensorflow.python.ops import init_ops\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a= 0.9*tf.complex(1.0, 1.0)\n",
    "mag= tf.abs(a)\n",
    "conj= tf.conj(a)\n",
    "phase= tf.atan2(tf.real(a), tf.imag(a))\n",
    "c= tf.cond(0 <= phase, lambda: a, lambda: tf.complex(0.,0.))\n",
    "\n",
    "print(tf.Variable(tf.constant(0,shape=[2,2])).get_shape()[1])\n",
    "\n",
    "b= tf.complex(tf.constant([1.,2.]), tf.constant([2.,2.]))\n",
    "s= tf.reduce_mean(b)\n",
    "\n",
    "d= {\"a\":a, \"b\":b}\n",
    "\n",
    "running_mean = tf.get_variable(\"running_mean\", shape=[1, 1, 1, 10], dtype=tf.complex64,\n",
    "                               trainable=False, initializer= init_ops.zeros_initializer())\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(mag.eval(), a.eval(), conj.eval(), phase.eval())\n",
    "    print(b.eval(), s.eval())\n",
    "    print(d['a'].eval())\n",
    "    print(running_mean.eval())\n",
    "    print(tf.cast(tf.constant(1.0), tf.complex64).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import init_ops\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math as m\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\"\"\" \n",
    "수정사항\n",
    "1. 숫자*complex tensor는 가능하나 float tensor * complex tensor는 작동 안함 ( ex. z/mag는 작동안함)\n",
    "    => tf.cast(float tensor, tf.complex64) 하면 r+0j complex tensor됨\n",
    "2. tf.complex(real,imag) = real+imag j (complex tensor) 이용 \n",
    "3. tf.complex_abs 대신 tf.abs 이용. 다른 버젼에선 tf.abs(z)가 작동 안할 수도 있음\n",
    "4. tf.cond(a<z<b)는 tf.cond(a<z & z<b)식으로 바꿔야 함. 다만 phase는 [-pi/2, pi/2]라서 0<phase <=> 0<phase<pi/2\n",
    "5. cnn filter 만들 때 첨부터 complex filter 하나를 만들던지 \n",
    "   float filter 2개를 만들어야 함. 기존의 함수는 complex 2개 만들어서 일단 float 2개로 수정함\n",
    "   (deep complex net 코드에서는 아예 layer value들이 real, imag 파트를 따로 만들뒤 concat 했는데 \n",
    "   우리처럼 아예 complex로 하는거랑 이거랑 속도차이가 날지 잘 몰겠음)\n",
    "   \n",
    "추가사항\n",
    "1. complex_standardization (function)\n",
    "   - input, shift, scaling, reshift, rescaling 값 받아서 계산만함\n",
    "2. complex_batch_normalization (function)\n",
    "   - input, is_training 받아서 variable 만들고 batch norm 시행 \n",
    "   - output batch_norm 결과\n",
    "\n",
    "앞으로 개선사항\n",
    "1. tf.cond 미분가능한지 확인\n",
    "2. threshold relu\n",
    "3. front end - fft filter 수정\n",
    "4. weight initializer 적용\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mag_phase(z):\n",
    "    mag = tf.abs(z)                             # return : tf.float32\n",
    "    phase = tf.atan2(tf.real(z), tf.imag(z))    # return : [-pi/2, pi/2]\n",
    "    return mag, phase\n",
    "\n",
    "\n",
    "def real_imag(z):\n",
    "    r = tf.real(z)\n",
    "    i = tf.imag(z)\n",
    "    return r, i\n",
    "\n",
    "\n",
    "def modrelu(z, reuse=False):\n",
    "    with tf.variable_scope(\"modReLU\", reuse=reuse):\n",
    "        b = tf.Variable(1, name='radius')\n",
    "        mag = tf.abs(z)\n",
    "        # tf.cast(real, tf.complex 64)=> real+0j\n",
    "        output = tf.multiply(tf.cast(tf.nn.relu(mag+b)/mag, tf.complex64), z)\n",
    "    return output\n",
    "\n",
    "\n",
    "def crelu(z, reuse=False):\n",
    "    with tf.variable_scope(\"CReLU\", reuse=reuse):\n",
    "        r,i = real_imag(z)\n",
    "        r = tf.nn.relu(r)   # do relu for each components\n",
    "        i = tf.nn.relu(i)\n",
    "    return tf.complex(r,i)\n",
    "\n",
    "\n",
    "def zrelu(z, reuse=False):\n",
    "    with tf.variable_scope(\"zReLU\", reuse=reuse):\n",
    "        mag, phase = mag_phase(z)\n",
    "        # 0<phase<pi does not work(phase is in [-pi/2, pi/2] so just 0<= phase works.)\n",
    "        output = tf.cond(0 <= phase, lambda: z, lambda: tf.complex(0., 0.))\n",
    "    return output\n",
    "\n",
    "def complex_standardization(input,\n",
    "                            filter_type= 'NWHC',\n",
    "                            shift = None,\n",
    "                            scale= None,\n",
    "                            beta= None,\n",
    "                            gamma= None,\n",
    "                            epsilon= 1e-4\n",
    "                            ):\n",
    "    \"\"\"\n",
    "    implement complex standardization and rescaling\n",
    "\n",
    "    Args:\n",
    "        input: input of the batchnorm ( NxWxHx2C)\n",
    "        shift: shift standardization tensor. [1,1,1,2*c] \n",
    "        scale: scaling standardization tensor dict. {'Vrr':, 'Vri':, 'Vii':}, each should preserve dimension of input\n",
    "        beta: reshift tensor.[1,1,1,2*c]\n",
    "        gamma: rescaling standardization tensor dict. {'Vrr':, 'Vri':, 'Vir':, 'Vii':},\n",
    "               each should preserve dimension of input\n",
    "\n",
    "    Return:\n",
    "        The output of complex standardization and rescaling\n",
    "\n",
    "    Raises:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    shape = input.get_shape()   # changed from tf.get_shape(input) which does not work\n",
    "    c = int(int(input.get_shape()[-1]) / 2)  # channel number\n",
    "\n",
    "    input_r_centered = input[:, :, :, :c] - shift[:, :, :, :c]\n",
    "    input_i_centered = input[:, :, :, c:] - shift[:, :, :, c:]\n",
    "\n",
    "    # to make matrix invertible add epsilon (epsilon=1e-4 according to the github page of authors)\n",
    "    Vrr = scale[\"Vrr\"] + epsilon\n",
    "    Vii = scale[\"Vii\"] + epsilon\n",
    "    Vri = scale[\"Vri\"] + epsilon\n",
    "\n",
    "    # We require the covariance matrix's inverse square root. That first requires\n",
    "    # square rooting, followed by inversion (I do this in that order because during\n",
    "    # the computation of square root we compute the determinant we'll need for\n",
    "    # inversion as well.\n",
    "\n",
    "    # tau = Vrr + Vii = Trace. Guaranteed >= 0 because SPD\n",
    "    tau = Vrr + Vii\n",
    "    # delta = (Vrr * Vii) - (Vri ** 2) = Determinant. Guaranteed >= 0 because SPD\n",
    "    delta = (Vrr * Vii) - (Vri ** 2)\n",
    "    s = tf.sqrt(delta)  # Determinant of square root matrix\n",
    "    t = tf.sqrt(tau + 2 * s)\n",
    "\n",
    "    # The square root matrix could now be explicitly formed as\n",
    "    #       [ Vrr+s Vri   ]\n",
    "    # (1/t) [ Vir   Vii+s ]\n",
    "    # https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix\n",
    "    # but we don't need to do this immediately since we can also simultaneously\n",
    "    # invert. We can do this because we've already computed the determinant of\n",
    "    # the square root matrix, and can thus invert it using the analytical\n",
    "    # solution for 2x2 matrices\n",
    "    #      [ A B ]             [  D  -B ]\n",
    "    # inv( [ C D ] ) = (1/det) [ -C   A ]\n",
    "    # http://mathworld.wolfram.com/MatrixInverse.html\n",
    "    # Thus giving us\n",
    "    #           [  Vii+s  -Vri   ]\n",
    "    # (1/s)(1/t)[ -Vir     Vrr+s ]\n",
    "    # So we proceed as follows:\n",
    "\n",
    "    inverse_st = 1.0 / (s * t)\n",
    "    Wrr = (Vii + s) * inverse_st\n",
    "    Wii = (Vrr + s) * inverse_st\n",
    "    Wri = -Vri * inverse_st\n",
    "\n",
    "    # And we have computed the inverse square root matrix W = sqrt(V)\n",
    "    # Normalization. We multiply, x_normalized = W.x.\n",
    "    # The returned result will be a complex standardized input\n",
    "    # where the real and imaginary parts are obtained as follows:\n",
    "\n",
    "    # x_real_normed = Wrr * x_real_centred + Wri * x_imag_centred\n",
    "    input_r_normed = Wrr*input_r_centered + Wri*input_i_centered\n",
    "    # x_imag_normed = Wri * x_real_centred + Wii * x_imag_centred\n",
    "    input_i_normed = Wri*input_r_centered + Wii*input_i_centered\n",
    "\n",
    "    # rescaling and shift using inputs(beta & gamma)\n",
    "    output_r = gamma[\"Vrr\"]*input_r_normed + gamma[\"Vri\"]*input_i_normed\n",
    "    output_i = gamma[\"Vir\"]*input_r_normed + gamma[\"Vii\"]*input_i_normed\n",
    "    output = tf.concat([output_r, output_i], axis=-1) + beta\n",
    "\n",
    "    # change filter axes\n",
    "    if filter_type == 'NCWH':\n",
    "        output= tf.transpose(output, [0, 3, 1, 2])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def complex_batch_normalization(name,\n",
    "                                input,\n",
    "                                is_training= True,\n",
    "                                momentum= 0.99,\n",
    "                                reuse=False,\n",
    "                                centering= True,\n",
    "                                scaling= True,\n",
    "                                beta_initializer= init_ops.zeros_initializer(),\n",
    "                                gamma_initializer= init_ops.random_normal_initializer(),\n",
    "                                dtype_float=tf.float32,\n",
    "                                epsilon= 1e-4,\n",
    "                                filter_type= \"NWHC\"\n",
    "                                ):\n",
    "    \"\"\"\n",
    "    implement complex batch normalization\n",
    "    \n",
    "    Args:\n",
    "        name: name of the batchnorm variable scope\n",
    "        input: input of the batchnorm (NxWxHx2C)\n",
    "        is_training: whether training time or test time\n",
    "        momentum: running mean and variance decay rate\n",
    "        \n",
    "    Return:\n",
    "        The output of batch normalization\n",
    "\n",
    "    Raises:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if filter_type == 'NCWH':   # change filter axes\n",
    "        input= tf.transpose(input, [0, 2, 3, 1])\n",
    "\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        c = int(int(input.get_shape()[-1])/2)   # channel number\n",
    "\n",
    "        beta = tf.get_variable(\"beta\", shape= [1, 1, 1, 2*c], dtype= dtype_float, initializer=beta_initializer)\n",
    "        # for scaling gamma need not to be symmetric\n",
    "        gamma_rr = tf.get_variable(\"gamma_rr\", shape= [1, 1, 1, c], dtype= dtype_float, initializer=gamma_initializer)\n",
    "        gamma_ri = tf.get_variable(\"gamma_ri\", shape=[1, 1, 1, c], dtype=dtype_float, initializer=gamma_initializer)\n",
    "        gamma_ir = tf.get_variable(\"gamma_ir\", shape=[1, 1, 1, c], dtype=dtype_float, initializer=gamma_initializer)\n",
    "        gamma_ii = tf.get_variable(\"gamma_ii\", shape=[1, 1, 1, c], dtype=dtype_float, initializer=gamma_initializer)\n",
    "        gamma= {'Vrr': gamma_rr, 'Vri': gamma_ri, 'Vir': gamma_ir, 'Vii': gamma_ii}\n",
    "\n",
    "        running_mean = tf.get_variable(\"running_mean\", shape=[1, 1, 1, 2*c], dtype=dtype_float,\n",
    "                                       trainable=False, initializer= init_ops.zeros_initializer())\n",
    "        running_Vrr = tf.get_variable(\"running_Var_rr\", shape=[1, 1, 1, c], dtype=dtype_float,\n",
    "                                       trainable=False, initializer= init_ops.zeros_initializer())\n",
    "        running_Vri = tf.get_variable(\"running_Var_ri\", shape=[1, 1, 1, c], dtype=dtype_float,\n",
    "                                       trainable=False, initializer= init_ops.zeros_initializer())\n",
    "        running_Vii = tf.get_variable(\"running_Var_ii\", shape=[1, 1, 1, c], dtype=dtype_float,\n",
    "                                       trainable=False, initializer= init_ops.zeros_initializer()\n",
    "                                      )\n",
    "\n",
    "        if is_training:\n",
    "            # average across the axes except feature axis, shape: the number of feature map\n",
    "            # at training step shift input by mini-batch mean\n",
    "            mean = tf.reduce_mean(input, axis=[0, 1, 2], keep_dims=True)\n",
    "\n",
    "            # at training step scale input by mini-batch variance\n",
    "            input_r_centered = input[:, :, :, :c] - mean[:, :, :, :c]\n",
    "            input_i_centered = input[:, :, :, c:] - mean[:, :, :, c:]\n",
    "            Vrr = tf.reduce_mean(tf.square(input_r_centered), axis=[0, 1, 2], keep_dims=True)\n",
    "            Vii = tf.reduce_mean(tf.square(input_i_centered), axis=[0, 1, 2], keep_dims=True)\n",
    "            Vri = tf.reduce_mean(tf.multiply(input_r_centered, input_i_centered), axis=[0, 1, 2], keep_dims=True)\n",
    "            minibatch_var = {'Vrr': Vrr, 'Vri': Vri, 'Vii': Vii}\n",
    "\n",
    "            # make node which update running mean\n",
    "            update_node= [tf.assign(running_mean, momentum * running_mean + (1 - momentum) * mean),\n",
    "                          tf.assign(running_Vrr, momentum * running_Vrr + (1 - momentum) * Vrr),\n",
    "                          tf.assign(running_Vri, momentum * running_Vri + (1 - momentum) * Vri),\n",
    "                          tf.assign(running_Vii, momentum * running_Vii + (1 - momentum) * Vii),\n",
    "                          ]\n",
    "\n",
    "            # before batch nomalization when forward pass, running mean, var should be updated\n",
    "            with tf.control_dependencies(update_node):\n",
    "                return complex_standardization(input, shift = mean, scale= minibatch_var,\n",
    "                                               beta= beta, gamma= gamma)\n",
    "        else :\n",
    "            # at test step shift input by running mean\n",
    "            # at test step scale input by running variance\n",
    "            running_var = {'Vrr': running_Vrr, 'Vri': running_Vri, 'Vii': running_Vii}\n",
    "\n",
    "            return complex_standardization(input, shift= running_mean, scale=running_var,\n",
    "                                           beta=beta, gamma=gamma)\n",
    "\n",
    "\n",
    "def front_end_cnn(name, input, N=1024, stride=256, reuse=False):\n",
    "    \"\"\"\n",
    "    Front end convolutional layer\n",
    "\n",
    "    Args:\n",
    "        name: name of the convnet layer\n",
    "        input: input of the convnet layer\n",
    "        N: \"dft size\" or \"output channel\"\n",
    "        stride: \"hop size\" or \"stride size\"\n",
    "        reuse: reuse layer(True) or not(False)\n",
    "\n",
    "    Return:\n",
    "        The output of the front end convnet layer\n",
    "\n",
    "    Raises:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    I = np.eye(N)\n",
    "    W = np.fft.fft(I)  # DFT matrix\n",
    "\n",
    "\n",
    "    A = np.real(W[:, :int(N / 2) + 1])  # (filter_size=N, input_channel=1, output_channel=int(N/2)+1)\n",
    "    B = np.imag(W[:, :int(N / 2) + 1])  # (filter_size=N, input_channel=1, output_channel=int(N/2)+1)\n",
    "    A = A.astype(\"float32\")\n",
    "    B = B.astype(\"float32\")\n",
    "\n",
    "    A = np.expand_dims(A, axis=1)  # (513,513) -> (513,1,513)\n",
    "    B = np.expand_dims(B, axis=1)  # (513,513) -> (513,1,513)\n",
    "\n",
    "    print(A.shape)\n",
    "    print(B.shape)\n",
    "\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        \"\"\" Initialize the conv filter as the Real part of Fourier Basis : A\n",
    "                                        &\n",
    "        Initialize the conv filter as the Imag part of Fourier Basis : B \"\"\"\n",
    "\n",
    "        FB_R = tf.Variable(A, name=\"RealFilter\",\n",
    "                           dtype=tf.float32)  # (filter_size=1024, input_channel=1, output_channel=513)\n",
    "        FB_I = tf.Variable(B, name=\"ImagFilter\",\n",
    "                           dtype=tf.float32)  # (filter_size=1024, input_channel=1, output_channel=513)\n",
    "\n",
    "        Ax = tf.nn.conv1d(input, FB_R, stride=stride, padding='SAME', data_format=\"NHWC\")\n",
    "        Bx = tf.nn.conv1d(input, FB_I, stride=stride, padding='SAME', data_format=\"NHWC\")\n",
    "\n",
    "        R = tf.cast(Ax, tf.complex64)\n",
    "        I = tf.cast(Bx, tf.complex64)\n",
    "\n",
    "        output = R + 1j * I\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# when we use real, imag filter each should be float type\n",
    "def complex_2d_cnn(name, input, oc, f_h=3, f_w=3, s_h=1, s_w=1,\n",
    "                   padding=\"SAME\", dtype=tf.float32, reuse=False):\n",
    "    \"\"\"\n",
    "    2d complex CNN layer\n",
    "\n",
    "    Args:\n",
    "        input -- input of the CNN layer(complex number), h = x+iy\n",
    "        oc -- output channel\n",
    "        f_h -- height wise filter size (default : 3)\n",
    "        f_w -- width wise filter size (default : 3)\n",
    "        s_h -- height wise stride (default : 1)\n",
    "        s_w -- width wise stride (default : 1)\n",
    "        name -- name of the scope\n",
    "        padding -- how to pad (default : \"SAME\")\n",
    "        dtype -- data type (default : tf.complex64)\n",
    "        reuse -- whether to reuse the layer or not\n",
    "\n",
    "    Returns:\n",
    "        The output of the CNN layer\n",
    "\n",
    "    Raises:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    input_r = tf.real(input)  # x\n",
    "    input_i = tf.imag(input)  # y\n",
    "    strides = [1, s_h, s_w, 1]\n",
    "    ic = input.get_shape()[-1]  # input channel\n",
    "\n",
    "    with tf.variable_scope(name, reuse=reuse) as scope:\n",
    "        A = tf.get_variable(\"RealFilter\", shape=[f_h, f_w, ic, oc], dtype=dtype)\n",
    "        B = tf.get_variable(\"ImagFilter\", shape=[f_h, f_w, ic, oc], dtype=dtype)\n",
    "\n",
    "        Ax = tf.nn.conv2d(input_r, filter=A, strides=strides, padding=padding)  # A*x\n",
    "        Bx = tf.nn.conv2d(input_r, filter=B, strides=strides, padding=padding)  # B*x\n",
    "        Ay = tf.nn.conv2d(input_i, filter=A, strides=strides, padding=padding)  # A*y\n",
    "        By = tf.nn.conv2d(input_i, filter=B, strides=strides, padding=padding)  # B*y\n",
    "\n",
    "        output = tf.complex(Ax-Bx, Ay+By)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def bss_eval(sep, i, sources):\n",
    "    # Current target\n",
    "    from numpy import dot, linalg, log10\n",
    "    min_len = min([len(sep), len(sources[i])])\n",
    "    sources = sources[:, :min_len]\n",
    "    sep = sep[:min_len]\n",
    "    target = sources[i]\n",
    "\n",
    "    # Target contribution\n",
    "    s_target = target * dot(target, sep.T) / dot(target, target.T)\n",
    "\n",
    "    # Interference contribution\n",
    "    pse = dot(dot(sources, sep.T), \\\n",
    "              linalg.inv(dot(sources, sources.T))).T.dot(sources)\n",
    "    e_interf = pse - s_target\n",
    "\n",
    "    # Artifact contribution\n",
    "    e_artif = sep - pse;\n",
    "\n",
    "    # Interference + artifacts contribution\n",
    "    e_total = e_interf + e_artif;\n",
    "\n",
    "    # Computation of the log energy ratios\n",
    "    sdr = 10 * log10(sum(s_target ** 2) / sum(e_total ** 2));\n",
    "    sir = 10 * log10(sum(s_target ** 2) / sum(e_interf ** 2));\n",
    "    sar = 10 * log10(sum((s_target + e_interf) ** 2) / sum(e_artif ** 2));\n",
    "\n",
    "    # Done!\n",
    "\n",
    "    return (sdr, sir, sar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 2, 4)\n",
      "[<tf.Variable 'input:0' shape=(5, 2, 2, 4) dtype=float32_ref>, <tf.Variable 'test/beta:0' shape=(1, 1, 1, 4) dtype=float32_ref>, <tf.Variable 'test/gamma_rr:0' shape=(1, 1, 1, 2) dtype=float32_ref>, <tf.Variable 'test/gamma_ri:0' shape=(1, 1, 1, 2) dtype=float32_ref>, <tf.Variable 'test/gamma_ir:0' shape=(1, 1, 1, 2) dtype=float32_ref>, <tf.Variable 'test/gamma_ii:0' shape=(1, 1, 1, 2) dtype=float32_ref>, <tf.Variable 'test/running_mean:0' shape=(1, 1, 1, 4) dtype=float32_ref>, <tf.Variable 'test/running_Var_rr:0' shape=(1, 1, 1, 2) dtype=float32_ref>, <tf.Variable 'test/running_Var_ri:0' shape=(1, 1, 1, 2) dtype=float32_ref>, <tf.Variable 'test/running_Var_ii:0' shape=(1, 1, 1, 2) dtype=float32_ref>]\n",
      "[[[[ -5.60701191e-02   2.14401633e-01   2.95200616e-01   2.17532068e-01]\n",
      "   [  1.57093048e-01  -1.87089518e-01  -1.38581559e-01   2.88439184e-01]]\n",
      "\n",
      "  [[ -2.56835997e-01  -3.79964709e-03  -4.15384769e-02   2.66917318e-01]\n",
      "   [ -1.33663654e-01   1.65009499e-03  -2.30437711e-01  -1.69701949e-01]]]\n",
      "\n",
      "\n",
      " [[[ -2.20756218e-01   3.04144025e-02  -1.09295219e-01   1.76844537e-01]\n",
      "   [ -7.54696727e-02  -2.66222835e-01  -2.69570500e-01  -2.28266805e-01]]\n",
      "\n",
      "  [[ -3.11360657e-02   7.92372823e-02   6.04646504e-02  -5.01294732e-02]\n",
      "   [ -2.76459008e-01  -6.94966018e-02  -1.83415845e-01   1.71184272e-01]]]\n",
      "\n",
      "\n",
      " [[[  2.39977688e-01   3.10302585e-01   2.67648906e-01  -1.22752011e-01]\n",
      "   [  2.58285612e-01  -1.45516887e-01   3.85524929e-02   8.29828978e-02]]\n",
      "\n",
      "  [[  2.78319806e-01   5.25991023e-02  -2.95292884e-01   2.03393310e-01]\n",
      "   [  2.02804297e-01   2.15847403e-01   2.09398240e-01  -3.11844110e-01]]]\n",
      "\n",
      "\n",
      " [[[  2.57507652e-01  -2.29047239e-02  -3.03318411e-01  -2.82948524e-01]\n",
      "   [ -1.49459034e-01   2.33158082e-01  -1.89471766e-01   7.13700056e-02]]\n",
      "\n",
      "  [[ -9.49996561e-02   2.68659681e-01   2.73429304e-01  -2.75047183e-01]\n",
      "   [ -1.84367031e-01   4.20616567e-02  -2.95662850e-01  -2.85899550e-01]]]\n",
      "\n",
      "\n",
      " [[[  3.12665671e-01  -2.14583576e-02  -6.84128106e-02  -2.77850181e-01]\n",
      "   [  2.71084338e-01   1.19487196e-01  -9.47072655e-02  -1.19708791e-01]]\n",
      "\n",
      "  [[  1.15729302e-01  -1.47759914e-02  -9.65109468e-03   1.94211990e-01]\n",
      "   [ -7.76507556e-02   1.42946005e-01   9.87264812e-02   5.89489937e-05]]]]\n",
      "\n",
      "\n",
      " training time!\n",
      "[[[[-2.36955023  0.37563896 -5.20609283  1.33183455]\n",
      "   [ 0.72142363 -3.08012486  2.75741005  0.18552065]]\n",
      "\n",
      "  [[-0.33132952 -1.54471719 -3.76513767  0.74346411]\n",
      "   [ 1.044631    0.19791514  0.09556222 -0.63140666]]]\n",
      "\n",
      "\n",
      " [[[ 0.1542262  -0.92315137 -2.48520374  0.57296133]\n",
      "   [ 1.36191094 -1.69565678  1.318362   -1.73412538]]\n",
      "\n",
      "  [[-0.78609389  0.34698099 -2.06718421  0.01547294]\n",
      "   [ 0.59150994 -1.69240475 -2.31704903  0.21332008]]]\n",
      "\n",
      "\n",
      " [[[-1.89456129  2.45966649 -1.04321098  0.57365966]\n",
      "   [-0.35509247 -1.95116806  1.94226468 -0.32807943]]\n",
      "\n",
      "  [[ 1.88170779 -0.85078776  6.20676422  0.73353881]\n",
      "   [-1.54439127  2.4476018  -0.8255856  -0.35276532]]]\n",
      "\n",
      "\n",
      " [[[ 1.91447544  0.44423777  6.03363323 -1.0767895 ]\n",
      "   [ 0.75699914  1.09310079 -0.60037935  0.92947233]]\n",
      "\n",
      "  [[-2.26336884  2.72264338 -5.44884491 -0.05477047]\n",
      "   [ 1.42777205  0.97025126  0.22171521 -0.86410064]]]\n",
      "\n",
      "\n",
      " [[[ 0.40889525  0.43584836  3.92948246 -1.05557287]\n",
      "   [ 0.54249883  0.93658268  3.70666695 -0.06898801]]\n",
      "\n",
      "  [[-0.17559367 -1.34865475  0.67517173  0.4738937 ]\n",
      "   [-1.08606946  0.65619719 -3.12834644  0.39346051]]]]\n",
      "\n",
      "\n",
      " test time!\n",
      "[[[[-18.97535706   5.80573368 -42.93672943  10.65978622]\n",
      "   [ 10.77869797 -26.06206131  36.85855865   2.78987694]]\n",
      "\n",
      "  [[ -1.25768542 -11.98804665 -28.1558094    6.6572175 ]\n",
      "   [ 12.14509773   7.49341774  10.50051308  -4.26093197]]]\n",
      "\n",
      "\n",
      " [[[  3.43078804  -5.59816885 -15.3333168    5.2013545 ]\n",
      "   [ 15.4016695   -9.13933659  22.71669388 -12.19099617]]\n",
      "\n",
      "  [[ -4.20250607   7.82439709 -11.39995193   0.63264275]\n",
      "   [  7.16157389 -12.50944138 -13.56226444   2.65314722]]]\n",
      "\n",
      "\n",
      " [[[-12.93554688  27.55594444  -1.48822784   4.35680771]\n",
      "   [  1.39384699 -14.09983826  28.51472855  -1.40881634]]\n",
      "\n",
      "  [[ 22.17741585  -5.16943645  71.38017273   6.40722227]\n",
      "   [ -9.90656662  29.05606079   0.75391388  -2.70210075]]]\n",
      "\n",
      "\n",
      " [[[ 22.36424446  10.68568611  69.66851807  -7.71787405]\n",
      "   [  9.39884281  13.54053116   3.52303982   7.4124155 ]]\n",
      "\n",
      "  [[-18.2110405   31.2312088  -45.32274628  -0.49964952]\n",
      "   [ 15.40337658  15.46965122  11.84194088  -6.22874308]]]\n",
      "\n",
      "\n",
      " [[[  8.75751495  10.56639767  48.42520142  -7.55403709]\n",
      "   [  9.76047897  13.75068569  46.24351501  -0.15931153]]\n",
      "\n",
      "  [[  2.25815773  -9.59550858  15.97370911   4.55308294]\n",
      "   [ -7.23405886  10.19475174 -22.00702286   3.43627644]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "input= tf.get_variable(\"input\", shape= [5,2,2,4], dtype= tf.float32)\n",
    "output= complex_batch_normalization(\"test\", input, is_training= True)\n",
    "print(output.get_shape())\n",
    "\n",
    "tensors= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "print(tensors)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(input.eval())\n",
    "    #print(sess.run(tensors))\n",
    "    print(\"\\n\\n training time!\")\n",
    "    print(output.eval())\n",
    "\n",
    "    print(\"\\n\\n test time!\")\n",
    "    output= complex_batch_normalization(\"test\", input, is_training= False, reuse=True)\n",
    "    print(output.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "# compute gradients\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# b를 음수로만 제한하는거도 해볼만 한듯\n",
    "def mod_relu(x, y, reuse=False):\n",
    "    with tf.variable_scope(\"mod_ReLU\", dtype= tf.float32, reuse=reuse):\n",
    "        b = tf.Variable(0., name='radius')\n",
    "        mag = tf.sqrt(x**2 + y**2)\n",
    "        x= tf.nn.relu(mag+b)/mag*x\n",
    "        y= tf.nn.relu(mag+b)/mag*y\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def c_relu(x, y, dtype= tf.float32, reuse=False):\n",
    "    with tf.variable_scope(\"complex_ReLU\", reuse=reuse):\n",
    "        x = tf.nn.relu(x)   # do relu for each components\n",
    "        y = tf.nn.relu(y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def z_relu(x, y, dtype= tf.float32, reuse=False):\n",
    "    with tf.variable_scope(\"z_ReLU\", reuse=reuse):\n",
    "        mask = tf.cast(tf.greater(x,0.) & tf.greater(y,0.), dtype= dtype)\n",
    "        x= mask*x\n",
    "        y= mask*y\n",
    "    return x, y\n",
    "\n",
    "def t_relu(x, y, dtype= tf.float32, reuse=False):\n",
    "    with tf.variable_scope(\"threshold_relu\", reuse=reuse):\n",
    "        b = tf.Variable(0., name= 'radius')\n",
    "        mag = tf.sqrt(x**2 + y**2)\n",
    "        mask= tf.cast(tf.greater(mag, b), dtype= dtype)\n",
    "        x= mask*x\n",
    "        y= mask*y\n",
    "    return x, y\n",
    "        \n",
    "x= tf.Variable(1, dtype= tf.float32)\n",
    "b= tf.Variable(1., dtype= tf.float32)\n",
    "y= tf.cond(b<x, lambda: x, lambda: 0.)\n",
    "z1, z2= mod_relu(x, b)\n",
    "z= z1+z2\n",
    "z= tf.gradients(z,x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sub:0' shape=(2, 2, 2, 10) dtype=float32>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input= tf.get_variable(\"input\", shape= [2,2,2,10], dtype= tf.float32)\n",
    "c = int(int(input.get_shape()[-1])/2)\n",
    "\n",
    "mean = tf.reduce_mean(input, axis=[0, 1, 2], keep_dims=True)\n",
    "\n",
    "print(int(5/2))\n",
    "# at training step scale input by mini-batch variance\n",
    "input_r_centered = input[:,:,:,:2*c] - mean[:,:,:,:2*c]\n",
    "input_r_centered\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
